h1. Concurrency in Scala

* Runnable
* Callable
* Threads
* Executors
* ExecutorService
* Futures
** Stock Java
** Our own Futures
* Solutions
** Producer/Consumer
** Parrallel combinators
** Single-machine MapReduce


h2. Runnable/Callable

Runnable has a single method that returns no value.

<pre>
trait Runnable {
  def run(): Unit
}
</pre>

Callable is similar to run except that it returns a value

<pre>
trait Callable[V] {
  def call(): V
}
</pre>


h2. Threads

Scala concurrency is built on top of the Java concurrency model.

On Sun JVMs, with a IO-heavy workload, we can run tens of thousands of
threads on a single machine.

A Thread takes a Runnable. You have to call @start@ on a Thread in
order for it to run the Runnable.


<pre>
scala> val hello = new Thread(new Runnable {
  def run() {
    println("hello world")
  }
})
hello: java.lang.Thread = Thread[Thread-3,5,main]

scala> hello.start
hello world

</pre>

When you see a class implementing Runnable, you know it's intended
to run in a Thread somewhere by somebody.


h2. Executors

With the release of Java 5, it was decided that a more abstract
interface was required.


You can get an @ExecutorService@ using static methods on the @Executors@
object. Those methods provide you to configure an @ExecutorService@ with
a variety of policies such as thread pooling.


<pre>
import java.net.{Socket, ServerSocket}
import java.util.concurrent.{Executors, ExecutorService}
import java.util.Date

class NetworkService(port: Int, poolSize: Int) extends Runnable {
  val serverSocket = new ServerSocket(port)
  val pool: ExecutorService = Executors.newFixedThreadPool(poolSize)

  def run() {
    try {
      while (true) {
        // This will block until a connection comes in.
        val socket = serverSocket.accept()
        pool.execute(new Handler(socket))
      }
    } finally {
      pool.shutdown()
    }
  }
}

class Handler(socket: Socket) extends Runnable {
  def message = (Thread.currentThread.getName() + "\n").getBytes

  def run() {
    socket.getOutputStream.write(message)
    socket.getOutputStream.close()
  }
}

(new NetworkService(2020, 2)).run
</pre>

Here's a transcript connecting to it showing how the internal threads
are re-used.

<pre>
$ nc localhost 2020
pool-1-thread-1

$ nc localhost 2020
pool-1-thread-2

$ nc localhost 2020
pool-1-thread-1

$ nc localhost 2020
pool-1-thread-2
</pre>


h2. Thread Safety

<pre>
class Person(var name: String) {
  def set(changedName: String) {
    name = changedName
  }
}
</pre>

This program is not safe in a multi-threaded environment. If two
threads have references to the same instance of an Adder and call
@add@, you can't predict what @i@ will be at the end of both calls. It
could be 2, it could be 1!

h2. Three tools

h4. synchronization

Mutexes provide ownership semantics. When you enter a mutex, you own
it. The most common way of using a mutex in the JVM is by
synchronizing on something. In this case, we'll synchronize on our
userMap.

In the JVM, you can synchronize on any instance that's not null.

<pre>
class Person(var name: String) {
  def set(changedName: String) {
    name.synchronized {
      name = changedName
    }
  }
}
</pre>



h4. volatile

With Java 5's change to the memory model, volatile and synchronized
are basically identical except with volatile, nulls are allowed.

@synchronized@ allows for more fine-grained locking. @volatile@
synchronizes on every access.

<pre>
class Person(@volatile var name: String) {
  def set(changedName: String) {
    name = changedName
  }
}
</pre>

h4. AtomicReference

Also in Java 5, a whole raft of low-level concurrency primitives were added. One of them is an @AtomicReference@ class

<pre>
import java.util.concurrent.atomic.AtomicReference

class Person(val name: AtomicReference[String]) {
  def set(changedName: String) {
    name.set(changedName)
  }
}
</pre>

h4. Does this cost anything?

@AtomicReference is the most costly of these two choices since you
have to go through method dispatch to access values.

@volatile@ and @synchronized@ are built on top of Java's built-in
monitors. Monitors cost very little if there's no contention. Since
@synchronized@ allows you more fine-grained control over when you
synchronize, there will be less contention so @synchronized@ tends to
be the cheapest option.

PLEASE CORRECT ME IF I'M WRONG HERE. This is a complicated subject,
I'm sure there will be a lengthy classroom discussion at this point.


h2. Other neat tools from Java 5

As I mentioned with @AtomicReference@, Java 5 brought many great tools
along with it.

h2. Let's build an unsafe search engine

Here's a simple inverted index that isn't thread-safe. Our inverted
index maps parts of a name to a given User.

This is written in a naive way assuming only single-threaded access.

<pre>
import scala.collection.mutable

case class User(name: String, id: Int)

class InvertedIndex(val userMap: mutable.Map[String, User]) {

  def this() = this(new mutable.HashMap[String, User])

  def tokenizeName(name: String): Seq[String] = {
    name.split(" ").map(_.toLowerCase)
  }

  def add(term: String, user: User) {
    userMap += term -> user
  }

  def add(user: User) {
    tokenizeName(user.name).foreach { term =>
      add(term, user)
    }
  }
}
</pre>

I've left out how to get users out of our index for now. We'll get to
that later.

h2. Let's make it safe

In our inverted index example above, userMap is not guaranteed to be
safe. Multiple clients could try to add items at the same time and
have the same kinds of visibility errors we saw in our first @Person@
example.

Since userMap isn't thread-safe, how we do we keep only a single
thread at a time mutating it?

You might consider locking on userMap while adding.

<pre>
def add(user: User) {
  userMap.synchronized {
    tokenizeName(user.name).foreach { term =>
      add(term, user)
    }
  }
}
</pre>

Unfortunately, this is too coarse. Always try to do as much expensive
work outside of the mutex as possible. Remember what I said about
locking being cheap if there is no contention. If you do less work
inside of a block, there will be less contention.

<pre>
def add(user: User) {
  // tokenizeName was measured to be the most expensive operation.
  val tokens = tokenizeName(user.name)

  tokens.foreach { term =>
    userMap.synchronized {
      add(term, user)
    }
  }
}
</pre>

h2. SynchronizedMap

We can mixin synchronization with a mutable HashMap.


<pre>
import scala.collection.mutable.SynchronizedMap

class SynchronizedInvertedIndex(userMap: mutable.Map[String, User]) extends InvertedIndex(userMap) {
  def this() = this(new mutable.HashMap[String, User] with SynchronizedMap[String, User])
}
</pre>

h2. Java ConcurrentHashMap

Java comes with a nice thread-safe ConcurrentHashMap. We can use
JavaConversions to give us nice Scala semantics.

In fact, we can seamlessly layer our new, thread-safe InvertedIndex as
an extension of the old, error-prone one.

<pre>
import java.util.concurrent.ConcurrentHashMap
import scala.collection.JavaConversions._

class ConcurrentInvertedIndex(userMap: mutable.ConcurrentMap[String, User])
    extends InvertedIndex(userMap) {

  def this() = this(new ConcurrentHashMap[String, User])

  override def add(user: User) {
    tokenizeName(user.name).foreach { term => add(term, user) }
  }
}
</pre>

h2. Let's load our InvertedIndex

h3. The naive way

class FileRecordProducer(path: String) {
  def makeUser(line: String) = line.split(",") match {
    case Array(name, userid) => User(name, userid.trim().toInt)
  }

  def run() {
    Source.fromFile(path, "utf-8").getLines.foreach { line =>
      index.add(makeUser(line))
    }
  }
}

We can't read a file in parallel but we _can_ build the User and add
it to the index in parallel.

h3. A solution: Producer/Consumer

